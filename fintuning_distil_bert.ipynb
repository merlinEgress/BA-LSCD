{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388da12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "#!pip install evaluate\n",
    "#!pip install accelerate - U\n",
    "#!pip install transformers[torch]\n",
    "#!pip install peft\n",
    "#!pip install torch torchvision transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de073d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, GenerationConfig\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#anwenden des modells auf das erste fullText element\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import methods as mp\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b4e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "path_social_science = '/Users/merlin/Desktop/BA/Bachelorabeit Uni Leipzig/programming/dhox/data/1880_nopsy.jsonl'\n",
    "dataset =  pd.read_json(path_social_science, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e106b05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loade distilBert a smaller version of bert fÃ¼r kÃ¼rzere Laufzeit\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d8239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2d1197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['creator', 'datePublished', 'docSubType', 'docType', 'doi', 'fullText',\n",
       "       'id', 'identifier', 'isPartOf', 'issueNumber', 'keyphrase', 'language',\n",
       "       'outputFormat', 'pageCount', 'pageEnd', 'pageStart', 'pagination',\n",
       "       'provider', 'publicationYear', 'publisher', 'sequence', 'tdmCategory',\n",
       "       'title', 'url', 'volumeNumber', 'wordCount', 'unigramCount',\n",
       "       'bigramCount', 'trigramCount', 'sourceCategory', 'abstract'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39214efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "for i in range(2010,2021):\n",
    "    data = dataset[dataset['datePublished'].str[:4] == str(i)]\n",
    "    train_data= pd.concat([train_data, data], ignore_index=True)\n",
    "train_data['datePublished'] = pd.to_datetime(train_data['datePublished'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade7e6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[train_data['datePublished'].dt.year == 2020]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512b5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['fullText', 'id', 'datePublished','sourceCategory']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92178e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3350    INTRODUCTION TO THE SYMPOSIUM ON COVID-19, GLO...\n",
       "3351    55\\nCapitalist and Communal Foundations in The...\n",
       "3352    RESEARCH ARTICLE\\nCopyright Donets EV, Chudino...\n",
       "3353    Creative Commons Non Commercial CC BY-NC: This...\n",
       "3354    Quality of Life for Women with Human\\nPapillom...\n",
       "                              ...                        \n",
       "3825    Ø­ÙˆÙ„ Ø§Ù„Ø´Ø¨Ø§Ø¨ ÙˆØ§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„ÙˆØ·Ù†ÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø±Ø· Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ...\n",
       "3826    Research-based language learning through \\nfra...\n",
       "3827    Journal of Ethnic and Cultural Studies Copyrig...\n",
       "3828    42 RCC Perspectives Women and Energy 43 Introd...\n",
       "3829    Book Reviews 321 Johnhenry Gonzalez, Maroon Na...\n",
       "Name: fullText, Length: 480, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"fullText\"] = train_data[\"fullText\"].apply(mp.join_strings)\n",
    "train_data['fullText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20136683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4359 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data[\"token_count\"] = train_data[\"fullText\"].apply(lambda x: len(tokenizer(x)[\"input_ids\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d9ac5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4610471"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['token_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "848a46ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4610471"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"fullText\"] = train_data[\"fullText\"].apply(mp.join_strings)\n",
    "train_data[\"token_count\"] = train_data[\"fullText\"].apply(lambda x: len(tokenizer(x)[\"input_ids\"]))\n",
    "train_data['token_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e70f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16912"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = train_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a266b9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.58s/it]\n"
     ]
    }
   ],
   "source": [
    "chunked_df = preprocess_with_labels(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7daf6cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "chunked_dataset = chunked_df\n",
    "print(chunked_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74bcf3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "chunked_dataset = chunked_dataset.remove_columns(\n",
    "    [col for col in chunked_dataset.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
    ")\n",
    "print(chunked_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ed780cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5494ba18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'group_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grouped_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_chunks\u001b[49m(chunked_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'group_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_dataset = group_chunks(chunked_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda52694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3350"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a247919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f415d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class GroupedDataCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, features):\n",
    "        # Flatten chunks within each document\n",
    "        flat_features = []\n",
    "        for f in features:\n",
    "            for ids, mask in zip(f[\"input_ids\"], f[\"attention_mask\"]):\n",
    "                flat_features.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "        return super().torch_call(flat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fd9a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create a data collator that dynamically masks tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\"),\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,   # 15% masking, standard BERT objective\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fee5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `dataset` is your Hugging Face Dataset from preprocess_funktion_fast_list_of_lists\n",
    "chunked_dataset = chunked_dataset.remove_columns([c for c in chunked_dataset.column_names if c not in [\"input_ids\",\"attention_mask\",\"labels\"]])\n",
    "split_dataset = chunked_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cf446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 17871,  1997,  2740,  1004,  4200,  5285,  1012,  1017,  1010,\n",
       "          2053,  1012,  1017,  1010,  4903,  1012, 24398,  1516, 25797,  9193,\n",
       "          1024,  2184,  1012,  5401, 22932,  1013,  1054, 14227,  1012, 12609,\n",
       "          1012,  7886,  2629,  1075, 12609,  2118,  1997,  3516,  2811, 20300,\n",
       "         18046,  1998,  4975, 20129,  3802, 15006,  1999,  3433,  2000,  2966,\n",
       "          3691,  1024,  2019,  4357,  2817,  1997,  2308,  2091,  8715, 13010,\n",
       "          6864,  1054,  1012,  7305,  1998, 11496, 10635,  1999,  2023,  3720,\n",
       "          1010,  2057,  7475,  2008,  2091,  8715,  1006, 16233,  1007, 13010,\n",
       "          6224,  2000, 18793,  1999,  2966,  4654, 29206,  9623,  2024,  9996,\n",
       "         10959,  2011,  2037,  9501,  1997,  5022,  1998, 11572,  1012,  2000,\n",
       "          4337,  2023,  3291,  1010,  2057,  2424,  2008,  2070, 13010,  1010,\n",
       "          4919, 10756,  1997,  2336,  2007, 16233,  1010, 28667,  6279, 22139,\n",
       "          2037,  3802, 15006,  2083,  2048,  3078, 17871,  2389,  9942,  1024,\n",
       "         20300, 18046,  1998,  4975, 20129,  3802, 15006,  1012, 13010,  2024,\n",
       "          2583,  2000,  2062, 10350,  4009,  2013, 11062,  3691,  2043,  2027,\n",
       "          4338,  1996, 18046,  1997,  2037,  7696,  1010,  4526,  2047,  9356,\n",
       "          1998, 14879,  2037,  9501,  1012,  2006,  1996,  2060,  2192,  1010,\n",
       "          4748,  6767,  1011,  4937,  2229,  2064,  2036, 10439, 19500,  2966,\n",
       "          3691,  2011,  4975,  2019, 20129,  3802, 15006,  1999,  2029,  2027,\n",
       "          3443,  1996,  3785,  2005,  2037,  2219,  6577,  1998,  3749,  2037,\n",
       "          2219, 15251,  1012,  2083,  1996,  2224,  1997,  2122,  2048,  9942,\n",
       "          1010, 13010,  2024,  2583,  2000,  4366,  2037, 13433,  5332,  1011,\n",
       "         14841,  5644,  2004, 10756,  1010,  2096,  2145,  8754, 17441,  3893,\n",
       "          3802, 15006,  1012,  3145, 22104,  1024, 11980,  1010, 20129, 17871,\n",
       "          1010, 20129,  3802, 15006,  1010, 28667,  6279, 25284,  3802, 15006,\n",
       "          1010,  2740, 12288,  7305,  1998, 10635, 25191,  5674,  2008,  2017,\n",
       "          2024,  4397,  6875,  1012,  3383,  2017,  2024,  2702,  3134,  2247,\n",
       "          1012,  1996, 10032,  2001,  3740,  1010,  1998,  2017,  1521,  2128,\n",
       "          2525,  3241,  2055, 18066,  1037,  2047,  3336,  2046,  1996,  2155,\n",
       "          1012,  2017,  1521,  2310,  6684, 10426,  2000,  2115,  2047,  2110,\n",
       "          2043,  2115,  3460,  5176,  2065,  2017,  1521,  1040,  2066,  3653,\n",
       "         19833,  2389, 11326,  2005,  2091,  8715,  1006, 16233,  1007,  1998,\n",
       "          2060, 10381, 21716, 27642,  3785,  1012,  1996,  3231,  1010,  2016,\n",
       "          2758,  1010,  2003,  2512,  2378, 12044,  3512,  1998, 22382,  2053,\n",
       "          3891,  2000,  1996,  3336,  1012,  2017,  5993,  2000,  2022, 12238,\n",
       "          1010,  3241,  2008,  2009,  1521,  1055,  2488,  2000,  2031,  2062,\n",
       "          2592,  2084,  2625,  1012,  6600,  1010,  2017,  2024,  6135,  4895,\n",
       "         28139, 19362,  2098,  2005,  1996,  2739,  2008,  1996,  3336,  3497,\n",
       "          2038, 16233,  1010,  1037, 10928,  2008,  2003,  4484,  2083, 17503,\n",
       "          1010, 16474,  5604,  1012,  3402,  1010,  2115,  3460,  2003,  3331,\n",
       "          2055,  2115, 15124,  7047,  1012,  2016,  2758,  2017,  2453,  2215,\n",
       "          2000,  5136, 23552,  1996, 10032,  1012,  2030,  3383,  2017,  1521,\n",
       "          1040,  2066,  2000,  2191,  2019,  9886,  2933,  1012, 11968,  1011,\n",
       "          4372,  3436,  1996,  2775,  2003,  2053,  2936,  1996,  3653, 17421,\n",
       "         16790,  1012,  2115,  3460,  2003,  5776,  1998,  2785,  1010,  2021,\n",
       "         10350, 14456,  2016,  2003,  2025,  2019,  6739,  1999, 16233,  1012,\n",
       "          1999,  2755,  1010,  2016,  1521,  1055,  2196,  5359,  1037,  3336,\n",
       "          2007, 16233,  2487,  2077,  1012,  1999,  2023,  2617,  3561,  2007,\n",
       "          6724,  1998,  3571,  1010,  2073,  2064,  2017,  2735,  2005,  3737,\n",
       "          2592,  1517,  2025,  2074,  2055,  1996,  2966, 12763,  3378,  2007,\n",
       "         16233,  2021,  2036,  2055,  2054,  2166,  3504,  2066,  2005,  2111,\n",
       "          2007,   102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([  101, 17871,  1997,  2740,  1004,  4200,  5285,  1012,  1017,  1010,\n",
       "          2053,  1012,  1017,  1010,  4903,  1012, 24398,  1516, 25797,  9193,\n",
       "          1024,  2184,  1012,  5401, 22932,  1013,  1054, 14227,  1012, 12609,\n",
       "          1012,  7886,  2629,  1075, 12609,  2118,  1997,  3516,  2811, 20300,\n",
       "         18046,  1998,  4975, 20129,  3802, 15006,  1999,  3433,  2000,  2966,\n",
       "          3691,  1024,  2019,  4357,  2817,  1997,  2308,  2091,  8715, 13010,\n",
       "          6864,  1054,  1012,  7305,  1998, 11496, 10635,  1999,  2023,  3720,\n",
       "          1010,  2057,  7475,  2008,  2091,  8715,  1006, 16233,  1007, 13010,\n",
       "          6224,  2000, 18793,  1999,  2966,  4654, 29206,  9623,  2024,  9996,\n",
       "         10959,  2011,  2037,  9501,  1997,  5022,  1998, 11572,  1012,  2000,\n",
       "          4337,  2023,  3291,  1010,  2057,  2424,  2008,  2070, 13010,  1010,\n",
       "          4919, 10756,  1997,  2336,  2007, 16233,  1010, 28667,  6279, 22139,\n",
       "          2037,  3802, 15006,  2083,  2048,  3078, 17871,  2389,  9942,  1024,\n",
       "         20300, 18046,  1998,  4975, 20129,  3802, 15006,  1012, 13010,  2024,\n",
       "          2583,  2000,  2062, 10350,  4009,  2013, 11062,  3691,  2043,  2027,\n",
       "          4338,  1996, 18046,  1997,  2037,  7696,  1010,  4526,  2047,  9356,\n",
       "          1998, 14879,  2037,  9501,  1012,  2006,  1996,  2060,  2192,  1010,\n",
       "          4748,  6767,  1011,  4937,  2229,  2064,  2036, 10439, 19500,  2966,\n",
       "          3691,  2011,  4975,  2019, 20129,  3802, 15006,  1999,  2029,  2027,\n",
       "          3443,  1996,  3785,  2005,  2037,  2219,  6577,  1998,  3749,  2037,\n",
       "          2219, 15251,  1012,  2083,  1996,  2224,  1997,  2122,  2048,  9942,\n",
       "          1010, 13010,  2024,  2583,  2000,  4366,  2037, 13433,  5332,  1011,\n",
       "         14841,  5644,  2004, 10756,  1010,  2096,  2145,  8754, 17441,  3893,\n",
       "          3802, 15006,  1012,  3145, 22104,  1024, 11980,  1010, 20129, 17871,\n",
       "          1010, 20129,  3802, 15006,  1010, 28667,  6279, 25284,  3802, 15006,\n",
       "          1010,  2740, 12288,  7305,  1998, 10635, 25191,  5674,  2008,  2017,\n",
       "          2024,  4397,  6875,  1012,  3383,  2017,  2024,  2702,  3134,  2247,\n",
       "          1012,  1996, 10032,  2001,  3740,  1010,  1998,  2017,  1521,  2128,\n",
       "          2525,  3241,  2055, 18066,  1037,  2047,  3336,  2046,  1996,  2155,\n",
       "          1012,  2017,  1521,  2310,  6684, 10426,  2000,  2115,  2047,  2110,\n",
       "          2043,  2115,  3460,  5176,  2065,  2017,  1521,  1040,  2066,  3653,\n",
       "         19833,  2389, 11326,  2005,  2091,  8715,  1006, 16233,  1007,  1998,\n",
       "          2060, 10381, 21716, 27642,  3785,  1012,  1996,  3231,  1010,  2016,\n",
       "          2758,  1010,  2003,  2512,  2378, 12044,  3512,  1998, 22382,  2053,\n",
       "          3891,  2000,  1996,  3336,  1012,  2017,  5993,  2000,  2022, 12238,\n",
       "          1010,  3241,  2008,  2009,  1521,  1055,  2488,  2000,  2031,  2062,\n",
       "          2592,  2084,  2625,  1012,  6600,  1010,  2017,  2024,  6135,  4895,\n",
       "         28139, 19362,  2098,  2005,  1996,  2739,  2008,  1996,  3336,  3497,\n",
       "          2038, 16233,  1010,  1037, 10928,  2008,  2003,  4484,  2083, 17503,\n",
       "          1010, 16474,  5604,  1012,  3402,  1010,  2115,  3460,  2003,  3331,\n",
       "          2055,  2115, 15124,  7047,  1012,  2016,  2758,  2017,  2453,  2215,\n",
       "          2000,  5136, 23552,  1996, 10032,  1012,  2030,  3383,  2017,  1521,\n",
       "          1040,  2066,  2000,  2191,  2019,  9886,  2933,  1012, 11968,  1011,\n",
       "          4372,  3436,  1996,  2775,  2003,  2053,  2936,  1996,  3653, 17421,\n",
       "         16790,  1012,  2115,  3460,  2003,  5776,  1998,  2785,  1010,  2021,\n",
       "         10350, 14456,  2016,  2003,  2025,  2019,  6739,  1999, 16233,  1012,\n",
       "          1999,  2755,  1010,  2016,  1521,  1055,  2196,  5359,  1037,  3336,\n",
       "          2007, 16233,  2487,  2077,  1012,  1999,  2023,  2617,  3561,  2007,\n",
       "          6724,  1998,  3571,  1010,  2073,  2064,  2017,  2735,  2005,  3737,\n",
       "          2592,  1517,  2025,  2074,  2055,  1996,  2966, 12763,  3378,  2007,\n",
       "         16233,  2021,  2036,  2055,  2054,  2166,  3504,  2066,  2005,  2111,\n",
       "          2007,   102])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6802ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM\n",
    "\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba53a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dapt-distilbert\",\n",
    "    overwrite_output_dir=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5,\n",
    "    logging_steps=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=5,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,             # ðŸ‘ˆ CPU safe\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "training_args.use_cpu=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01bd3f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/jz3xgqts0zx_lxl_9ylrxrv80000gn/T/ipykernel_53071/1409845953.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import wandb\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\"),\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e04fc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 02:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.369600</td>\n",
       "      <td>2.544536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.414500</td>\n",
       "      <td>2.689576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.630500</td>\n",
       "      <td>2.777219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.691400</td>\n",
       "      <td>2.520288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.510400</td>\n",
       "      <td>2.560085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.508800</td>\n",
       "      <td>2.672796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.492700</td>\n",
       "      <td>2.535910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.485200</td>\n",
       "      <td>2.559680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.462100</td>\n",
       "      <td>2.613721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.501200</td>\n",
       "      <td>2.441816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.666600</td>\n",
       "      <td>2.511365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=57, training_loss=2.434505169851738, metrics={'train_runtime': 139.2839, 'train_samples_per_second': 3.274, 'train_steps_per_second': 0.409, 'total_flos': 60447887179776.0, 'train_loss': 2.434505169851738, 'epoch': 1.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "789ff062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/merlin/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 2.4412734508514404\n",
      "Perplexity: 11.487660403031047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "import numpy as np\n",
    "print(\"Eval loss:\", eval_results[\"eval_loss\"])\n",
    "print(\"Perplexity:\", np.exp(eval_results[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "194c00f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.txt',\n",
       " './fine_tuned_model/added_tokens.json',\n",
       " './fine_tuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save trained model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d76ab500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./fine_tuned_model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load them back when needed\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1749226b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression (2911169554.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    tensorboard --logdir=./logs\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d406c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb305f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33080e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eafe5fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9024662125644bc9aa18ba9c88abec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969633b8e3c445f18982b1c094e3b6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86bdb3493a45e4842d70b679e4117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0, 'input_ids': [101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Check processed data\n",
    "print(encoded_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde60fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
